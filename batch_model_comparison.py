#!/usr/bin/env python3
"""
ğŸš€ æ‰¹é‡æ¨¡å‹æ¯”è¼ƒç³»çµ± - å…¨æ•¸æ“šé›†ç¶œåˆæ€§èƒ½è©•ä¼°
å°æ‰€æœ‰365d.csvæ­·å²æ•¸æ“šé€²è¡Œæ¨¡å‹æ¯”è¼ƒä¸¦è¨ˆç®—å¹³å‡æ€§èƒ½
"""

import os
import glob
import pandas as pd
import numpy as np
from pytorch_model_comparison import PyTorchModelComparison
import warnings
warnings.filterwarnings('ignore')

class BatchModelComparison:
    """æ‰¹é‡æ¨¡å‹æ¯”è¼ƒç³»çµ±"""
    
    def __init__(self, data_dir="/media/meow/Transcend/Quantum_curve_predict/free_historical_cache"):
        self.data_dir = data_dir
        self.results_all = []  # å­˜å„²æ‰€æœ‰æ•¸æ“šé›†çµæœ
        self.failed_datasets = []  # å­˜å„²å¤±æ•—çš„æ•¸æ“šé›†
        self.quantum_only = False  # é‡å­å°ˆç”¨æ¨¡å¼æ¨™èªŒ
        self.realtime_write = False  # å¯¦æ™‚å¯«å…¥æ¨¡å¼æ¨™èªŒ
        
    def set_quantum_only_mode(self, enabled=True):
        """è¨­ç½®é‡å­å°ˆç”¨æ¨¡å¼"""
        self.quantum_only = enabled
        if enabled:
            print("ğŸŒŒ å•Ÿç”¨é‡å­å°ˆç”¨æ¨¡å¼ - åƒ…è¨“ç·´QNNå’ŒQSVM-QNNæ¨¡å‹")
        else:
            print("ğŸ”§ æ¨™æº–æ¨¡å¼ - è¨“ç·´æ‰€æœ‰å¯ç”¨æ¨¡å‹")
    
    def set_realtime_write(self, enabled=True):
        """è¨­ç½®å¯¦æ™‚å¯«å…¥æ¨¡å¼"""
        self.realtime_write = enabled
        if enabled:
            print("ğŸ’¾ å•Ÿç”¨å¯¦æ™‚å¯«å…¥æ¨¡å¼ - æ¯å€‹æ¨¡å‹è¨“ç·´å®Œæˆå¾Œç«‹å³ä¿å­˜çµæœ")
        else:
            print("ğŸ“¦ æ‰¹é‡å¯«å…¥æ¨¡å¼ - æ¯5å€‹æ•¸æ“šé›†ä¿å­˜ä¸€æ¬¡ä¸­é–“çµæœ")
            
    def save_single_result(self, result_row):
        """ä¿å­˜å–®å€‹çµæœåˆ°CSV(å¯¦æ™‚å¯«å…¥æ¨¡å¼)"""
        if not self.realtime_write:
            return
            
        import pandas as pd
        import os
        
        # ä¿å­˜åˆ°å¯¦æ™‚çµæœæ–‡ä»¶ (ä¸é‡è¤‡æ·»åŠ åˆ°results_allï¼Œå› ç‚ºä¸»æµç¨‹å·²ç¶“æ·»åŠ äº†)
        realtime_file = "realtime_results.csv"
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‰µå»ºä¸¦å¯«å…¥æ¨™é¡Œ
        if not os.path.exists(realtime_file):
            df = pd.DataFrame([result_row])
            df.to_csv(realtime_file, index=False)
            print(f"ğŸ’¾ å‰µå»ºå¯¦æ™‚çµæœæ–‡ä»¶: {realtime_file}")
        else:
            # è¿½åŠ æ¨¡å¼å¯«å…¥
            df = pd.DataFrame([result_row])
            df.to_csv(realtime_file, mode='a', header=False, index=False)
            
        print(f"âœ… å¯¦æ™‚ä¿å­˜: {result_row['dataset']} - {result_row['model']}")
    
    def get_all_datasets(self):
        """ç²å–æ‰€æœ‰365d.csvæ•¸æ“šé›†æ–‡ä»¶"""
        pattern = os.path.join(self.data_dir, "*365d.csv")
        dataset_files = glob.glob(pattern)
        
        # æå–æ•¸æ“šé›†åç¨±ï¼ˆå»é™¤è·¯å¾‘å’Œå¾Œç¶´ï¼‰
        datasets = []
        for file_path in dataset_files:
            filename = os.path.basename(file_path)
            # æå–æ± åç¨±ï¼ˆå»é™¤å¾Œç¶´ï¼‰
            dataset_name = filename.replace('_365d.csv', '')
            datasets.append({
                'name': dataset_name,
                'file_path': file_path
            })
            
        return datasets
    
    def extract_pool_name(self, dataset_name):
        """å¾æ•¸æ“šé›†åç¨±æå–æ± åç¨±"""
        # è™•ç†ä¸åŒçš„å‘½åæ¨¡å¼
        if '_batch_historical' in dataset_name:
            return dataset_name.replace('_batch_historical', '')
        elif '_comprehensive_free_historical' in dataset_name:
            return dataset_name.replace('_comprehensive_free_historical', '')
        elif '_self_built_historical' in dataset_name:
            return dataset_name.replace('_self_built_historical', '')
        else:
            return dataset_name
    
    def run_single_dataset_comparison(self, dataset_info):
        """å°å–®å€‹æ•¸æ“šé›†é‹è¡Œæ¨¡å‹æ¯”è¼ƒ"""
        dataset_name = dataset_info['name']
        file_path = dataset_info['file_path']
        pool_name = self.extract_pool_name(dataset_name)
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ è™•ç†æ•¸æ“šé›†: {dataset_name}")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾‘: {file_path}")
        print(f"ğŸŠ æ± åç¨±: {pool_name}")
        print(f"{'='*80}")
        
        try:
            # å‰µå»ºæ¨¡å‹æ¯”è¼ƒå™¨ï¼Œå¢åŠ éŒ¯èª¤è™•ç†
            print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹æ¯”è¼ƒå™¨...")
            comparator = PyTorchModelComparison(pool_name=pool_name)
            
            # ç›´æ¥è®€å–æŒ‡å®šçš„CSVæ–‡ä»¶è€Œä¸æ˜¯å‹•æ…‹ä¸‹è¼‰
            if os.path.exists(file_path):
                print(f"ğŸ“Š è¼‰å…¥æ•¸æ“šé›†: {file_path}")
                # ä½¿ç”¨load_dataæ–¹æ³•è¼‰å…¥æ•¸æ“š
                success = comparator.load_data(file_path)
                if success:
                    print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(comparator.data)} æ¢è¨˜éŒ„")
                
                # é‹è¡Œå®Œæ•´æ¯”è¼ƒ
                try:
                    # æº–å‚™æ•¸æ“š
                    comparator.create_features()
                    comparator.prepare_data()
                    
                    # è¨“ç·´æ‰€æœ‰å¯ç”¨æ¨¡å‹ (å¯é€šéåƒæ•¸æ§åˆ¶)
                    if not hasattr(self, 'quantum_only') or not self.quantum_only:
                        comparator.train_random_forest()
                        comparator.train_xgboost()  # XGBoost
                    
                        # åˆ†åˆ¥å˜—è©¦æ¯å€‹PyTorchæ¨¡å‹
                        pytorch_models = [
                            ('LSTM', comparator.train_pytorch_lstm),
                            ('Transformer', comparator.train_pytorch_transformer),
                        ]
                        
                        for model_name, train_func in pytorch_models:
                            try:
                                import torch
                                test_tensor = torch.tensor([1.0])  # æ¸¬è©¦PyTorchæ˜¯å¦çœŸçš„å¯ç”¨
                                
                                print(f"ğŸ”¥ å˜—è©¦è¨“ç·´{model_name}æ¨¡å‹...")
                                results_before = len(comparator.results)
                                train_func()
                                results_after = len(comparator.results)
                                
                                # æª¢æŸ¥æ˜¯å¦çœŸçš„æ·»åŠ äº†çµæœ
                                if results_after > results_before:
                                    print(f"âœ… {model_name}æ¨¡å‹è¨“ç·´æˆåŠŸ")
                                else:
                                    print(f"âš ï¸ {model_name}æ¨¡å‹è¨“ç·´è¢«è·³é")
                            except Exception as e:
                                print(f"âš ï¸ è·³é{model_name}æ¨¡å‹: {e}")
                    
                    # é‡å­æ¨¡å‹ (ç¨ç«‹å˜—è©¦)
                    print("ğŸŒŒ é–‹å§‹é‡å­æ¨¡å‹è¨“ç·´...")
                    quantum_models = [
                        ('QNN', comparator.train_pytorch_qnn),
                        ('QSVM-QNN', comparator.train_pytorch_qsvmqnn),
                    ]
                    
                    for model_name, train_func in quantum_models:
                        try:
                            import pennylane as qml
                            # æª¢æŸ¥PyTorchæ˜¯å¦å¯ç”¨ (é‡å­æ¨¡å‹éœ€è¦PyTorch)
                            import torch
                            test_tensor = torch.tensor([1.0])  # æ¸¬è©¦PyTorchæ˜¯å¦çœŸçš„å¯ç”¨
                            
                            print(f"ğŸ”® å˜—è©¦è¨“ç·´{model_name}æ¨¡å‹...")
                            results_before = len(comparator.results)
                            train_func()
                            results_after = len(comparator.results)
                            
                            # æª¢æŸ¥æ˜¯å¦çœŸçš„æ·»åŠ äº†çµæœ
                            if results_after > results_before:
                                print(f"âœ… {model_name}æ¨¡å‹è¨“ç·´æˆåŠŸ")
                            else:
                                print(f"âš ï¸ {model_name}æ¨¡å‹è¨“ç·´è¢«è·³é")
                        except Exception as e:
                            print(f"âš ï¸ è·³é{model_name}æ¨¡å‹: {e}")
                    
                    # æ”¶é›†çµæœ
                    results = comparator.results
                    if results:
                        # ç‚ºæ¯å€‹æ¨¡å‹æ·»åŠ æ•¸æ“šé›†ä¿¡æ¯
                        for model_name, metrics in results.items():
                            result_row = {
                                'dataset': dataset_name,
                                'pool_name': pool_name, 
                                'model': model_name,
                                'test_mae': metrics['test_mae'],
                                'test_rmse': metrics['test_rmse'],
                                'test_direction_acc': metrics['test_direction_acc'],
                                'train_mae': metrics['train_mae'],
                                'train_rmse': metrics['train_rmse'], 
                                'train_direction_acc': metrics['train_direction_acc']
                            }
                            self.results_all.append(result_row)
                            self.save_single_result(result_row) # å¯¦æ™‚ä¿å­˜
                        
                        print(f"âœ… {dataset_name} å®Œæˆï¼æ”¶é›†äº† {len(results)} å€‹æ¨¡å‹çµæœ")
                    else:
                        print(f"âš ï¸ {dataset_name} æ²’æœ‰ç”Ÿæˆçµæœ")
                        self.failed_datasets.append(dataset_name)
                        
                except Exception as e:
                    print(f"âŒ {dataset_name} æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
                    self.failed_datasets.append(dataset_name)
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                self.failed_datasets.append(dataset_name)
                
        except Exception as e:
            print(f"âŒ {dataset_name} è™•ç†å¤±æ•—: {e}")
            self.failed_datasets.append(dataset_name)
    
    def run_batch_comparison(self, max_datasets=None):
        """é‹è¡Œæ‰¹é‡æ¨¡å‹æ¯”è¼ƒ"""
        print("ğŸŒŸ é–‹å§‹æ‰¹é‡æ¨¡å‹æ¯”è¼ƒ - å…¨æ•¸æ“šé›†ç¶œåˆè©•ä¼°")
        print("="*80)
        
        # ç²å–æ‰€æœ‰æ•¸æ“šé›†
        datasets = self.get_all_datasets()
        total_datasets = len(datasets)
        
        print(f"ğŸ“Š ç™¼ç¾ {total_datasets} å€‹æ•¸æ“šé›†æ–‡ä»¶")
        
        if max_datasets:
            datasets = datasets[:max_datasets]
            print(f"ğŸ¯ é™åˆ¶è™•ç†å‰ {max_datasets} å€‹æ•¸æ“šé›†")
        
        # è™•ç†æ¯å€‹æ•¸æ“šé›†
        for i, dataset_info in enumerate(datasets, 1):
            print(f"\nâ³ é€²åº¦: [{i}/{len(datasets)}] - {dataset_info['name']}")
            self.run_single_dataset_comparison(dataset_info)
            
            # æ¯è™•ç†5å€‹æ•¸æ“šé›†ä¿å­˜ä¸€æ¬¡ä¸­é–“çµæœ
            if i % 5 == 0 and self.results_all:
                print(f"ğŸ’¾ ä¿å­˜ä¸­é–“çµæœ (å·²è™•ç†{i}å€‹æ•¸æ“šé›†)...")
                self.save_intermediate_results(i)
        
        # ä¿å­˜è©³ç´°çµæœ
        if self.results_all:
            self.save_detailed_results()
            self.calculate_average_performance()
        
        # å ±å‘Šå¤±æ•—æƒ…æ³
        if self.failed_datasets:
            print(f"\nâš ï¸ å¤±æ•—çš„æ•¸æ“šé›† ({len(self.failed_datasets)}):")
            for failed in self.failed_datasets:
                print(f"  - {failed}")
        
        print(f"\nğŸ‰ æ‰¹é‡æ¯”è¼ƒå®Œæˆï¼")
        print(f"âœ… æˆåŠŸ: {len(self.results_all)//6 if self.results_all else 0} å€‹æ•¸æ“šé›†")  # å‡è¨­6å€‹æ¨¡å‹
        print(f"âŒ å¤±æ•—: {len(self.failed_datasets)} å€‹æ•¸æ“šé›†")
    
    def save_intermediate_results(self, processed_count):
        """ä¿å­˜ä¸­é–“çµæœ"""
        if self.results_all:
            results_df = pd.DataFrame(self.results_all)
            intermediate_file = f"intermediate_results_{processed_count}_datasets.csv"
            results_df.to_csv(intermediate_file, index=False)
            print(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜: {intermediate_file}")
        
    def save_detailed_results(self):
        """ä¿å­˜è©³ç´°çµæœåˆ°CSV"""
        if not self.results_all:
            print("âŒ æ²’æœ‰çµæœå¯ä¿å­˜")
            return
            
        results_df = pd.DataFrame(self.results_all)
        
        # ä¿å­˜è©³ç´°çµæœ
        detailed_file = "all_datasets_detailed_results.csv"
        results_df.to_csv(detailed_file, index=False)
        print(f"ğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜: {detailed_file}")
        
        # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“Š æ•¸æ“šé›†çµ±è¨ˆ:")
        print(f"  ç¸½è¨˜éŒ„æ•¸: {len(results_df)}")
        print(f"  æ•¸æ“šé›†æ•¸: {results_df['dataset'].nunique()}")
        print(f"  æ¨¡å‹æ•¸: {results_df['model'].nunique()}")
        print(f"  æ¨¡å‹é¡å‹: {list(results_df['model'].unique())}")
        
    def calculate_average_performance(self):
        """è¨ˆç®—ä¸¦é¡¯ç¤ºå¹³å‡æ€§èƒ½"""
        if not self.results_all:
            print("âŒ æ²’æœ‰çµæœå¯åˆ†æ")
            return
            
        results_df = pd.DataFrame(self.results_all)
        
        # æŒ‰æ¨¡å‹è¨ˆç®—å¹³å‡æ€§èƒ½
        avg_performance = results_df.groupby('model').agg({
            'test_mae': ['mean', 'std', 'count'],
            'test_rmse': ['mean', 'std'],  
            'test_direction_acc': ['mean', 'std'],
            'train_mae': ['mean', 'std'],
            'train_rmse': ['mean', 'std'],
            'train_direction_acc': ['mean', 'std']
        }).round(4)
        
        # ç°¡åŒ–åˆ—å
        avg_performance.columns = ['_'.join(col).strip() for col in avg_performance.columns]
        
        # ä¿å­˜å¹³å‡æ€§èƒ½çµæœ
        avg_file = "all_datasets_average_performance.csv" 
        avg_performance.to_csv(avg_file)
        print(f"ğŸ’¾ å¹³å‡æ€§èƒ½å·²ä¿å­˜: {avg_file}")
        
        # é¡¯ç¤ºæ’å
        print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å (æŒ‰æ¸¬è©¦æ–¹å‘æº–ç¢ºç‡):")
        ranking = results_df.groupby('model')['test_direction_acc'].mean().sort_values(ascending=False)
        
        for i, (model, acc) in enumerate(ranking.items(), 1):
            count = results_df[results_df['model'] == model].shape[0]
            std = results_df[results_df['model'] == model]['test_direction_acc'].std()
            print(f"  {i}. {model}: {acc:.2f}% (Â±{std:.2f}%, n={count})")
            
        # å‰µå»ºæ€§èƒ½æ‘˜è¦è¡¨
        summary_data = []
        for model in ranking.index:
            model_data = results_df[results_df['model'] == model]
            summary_data.append({
                'Model': model,
                'Avg_Test_Direction_Acc': model_data['test_direction_acc'].mean(),
                'Std_Test_Direction_Acc': model_data['test_direction_acc'].std(),
                'Avg_Test_MAE': model_data['test_mae'].mean(),
                'Std_Test_MAE': model_data['test_mae'].std(),
                'Avg_Test_RMSE': model_data['test_rmse'].mean(),
                'Std_Test_RMSE': model_data['test_rmse'].std(),
                'Dataset_Count': len(model_data)
            })
            
        summary_df = pd.DataFrame(summary_data)
        summary_file = "models_performance_summary.csv"
        summary_df.to_csv(summary_file, index=False)
        print(f"ğŸ’¾ æ€§èƒ½æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        return avg_performance, summary_df


def main():
    """ä¸»å‡½æ•¸ - é‹è¡Œæ‰¹é‡æ¯”è¼ƒ"""
    print("ğŸŒŸ å•Ÿå‹•å…¨æ•¸æ“šé›†æ‰¹é‡æ¨¡å‹æ¯”è¼ƒç³»çµ±")
    
    # å‰µå»ºæ‰¹é‡æ¯”è¼ƒå™¨
    batch_comparator = BatchModelComparison()
    
    # é‹è¡Œå…¨éƒ¨æ•¸æ“šé›†æ‰¹é‡æ¯”è¼ƒ
    print("\nğŸš€ ç”Ÿç”¢æ¨¡å¼: è™•ç†æ‰€æœ‰84å€‹æ•¸æ“šé›†...")
    # batch_comparator.run_batch_comparison()
    
    # å¦‚æœè¦æ¸¬è©¦æ¨¡å¼ï¼Œå–æ¶ˆè¨»é‡‹ä¸‹é¢é€™è¡Œ
    batch_comparator.run_batch_comparison(max_datasets=5)


if __name__ == "__main__":
    main() 