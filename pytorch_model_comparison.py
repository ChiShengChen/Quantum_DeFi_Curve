#!/usr/bin/env python3
"""
ğŸš€ ç´”PyTorch + é‡å­æ©Ÿå™¨å­¸ç¿’æ¨¡å‹æ¯”è¼ƒç³»çµ±
Random Forest vs LSTM vs Transformer vs QNN - åŒ…å«é‡å­ç¥ç¶“ç¶²çµ¡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# XGBoostå°å…¥
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    print("âœ… XGBoostå¯ç”¨")
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âŒ XGBoostä¸å¯ç”¨ï¼Œå°‡è·³éXGBoostæ¨¡å‹")

# PyTorchç›¸é—œå°å…¥
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    print("âœ… PyTorchå¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorchä¸å¯ç”¨ï¼Œå°‡åªä½¿ç”¨Random Forest")
    # å‰µå»ºç©ºçš„æ›¿ä»£é¡
    class nn:
        class Module: pass
        class LSTM: pass
        class Linear: pass
        class Dropout: pass
        class TransformerEncoderLayer: pass
        class TransformerEncoder: pass
        class MSELoss: pass
        class Parameter: pass

# PennyLaneé‡å­æ©Ÿå™¨å­¸ç¿’å°å…¥
try:
    import pennylane as qml
    import pennylane.numpy as pnp  # åªçµ¦PennyLaneä½¿ç”¨çš„numpy
    QML_AVAILABLE = True
    print("âœ… PennyLaneé‡å­æ©Ÿå™¨å­¸ç¿’åº«å¯ç”¨")
except ImportError:
    QML_AVAILABLE = False
    print("âŒ PennyLaneä¸å¯ç”¨ï¼Œå°‡è·³éQNNæ¨¡å‹")
    qml = None
    pnp = None

# PyTorchæ¨¡å‹å®šç¾©
if TORCH_AVAILABLE:
    class PyTorchLSTM(nn.Module):
        """PyTorch LSTMé æ¸¬æ¨¡å‹"""
        
        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
            super(PyTorchLSTM, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            
            self.lstm = nn.LSTM(
                input_size=input_size, 
                hidden_size=hidden_size, 
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True
            )
            self.dropout = nn.Dropout(dropout)
            self.fc = nn.Linear(hidden_size, 1)
            
        def forward(self, x):
            # LSTMå±¤
            lstm_out, _ = self.lstm(x)
            
            # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
            last_output = lstm_out[:, -1, :]
            
            # Dropoutå’Œå…¨é€£æ¥å±¤
            output = self.dropout(last_output)
            output = self.fc(output)
            
            return output

    class PyTorchTransformer(nn.Module):
        """PyTorch Transformeré æ¸¬æ¨¡å‹"""
        
        def __init__(self, input_size, d_model=64, nhead=8, num_layers=3, dropout=0.1):
            super(PyTorchTransformer, self).__init__()
            self.d_model = d_model
            
            # è¼¸å…¥æŠ•å½±
            self.input_projection = nn.Linear(input_size, d_model)
            
            # ä½ç½®ç·¨ç¢¼
            self.pos_embedding = nn.Parameter(torch.randn(1000, d_model))
            
            # Transformer Encoder
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_model * 4,
                dropout=dropout,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
            
            # è¼¸å‡ºå±¤
            self.dropout = nn.Dropout(dropout)
            self.fc = nn.Linear(d_model, 1)
            
        def forward(self, x):
            # è¼¸å…¥æŠ•å½±
            x = self.input_projection(x)
            
            # æ·»åŠ ä½ç½®ç·¨ç¢¼
            seq_len = x.size(1)
            pos_emb = self.pos_embedding[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)
            x = x + pos_emb
            
            # Transformerè™•ç†
            x = self.transformer(x)
            
            # å…¨å±€å¹³å‡æ± åŒ–
            x = x.mean(dim=1)
            
            # è¼¸å‡ºå±¤
            x = self.dropout(x)
            x = self.fc(x)
            
            return x

    class PyTorchQNN(nn.Module):
        """PyTorch + PennyLane é‡å­ç¥ç¶“ç¶²çµ¡æ¨¡å‹"""
        
        def __init__(self, input_size, n_qubits=4, n_layers=2):
            super(PyTorchQNN, self).__init__()
            self.input_size = input_size
            self.n_qubits = n_qubits
            self.n_layers = n_layers
            
            # æª¢æŸ¥PennyLaneæ˜¯å¦å¯ç”¨
            if not QML_AVAILABLE:
                raise ImportError("PennyLane not available for QNN")
            
            # å‰µå»ºé‡å­è¨­å‚™
            self.dev = qml.device("default.qubit", wires=n_qubits)
            
            # ä½¿ç”¨PCAé™ç¶­æ›¿ä»£ç¥ç¶“ç¶²çµ¡é è™•ç†
            # input_size ç¾åœ¨æ‡‰è©²ç­‰æ–¼ n_qubits * 2 (PCAé™ç¶­å¾Œçš„ç‰¹å¾µæ•¸)
            assert input_size == n_qubits * 2, f"PCAé™ç¶­å¾Œç‰¹å¾µæ•¸æ‡‰ç‚º{n_qubits * 2}ï¼Œä½†å¾—åˆ°{input_size}"
            print(f"âœ… QNNç›´æ¥ä½¿ç”¨PCAé™ç¶­å¾Œçš„{input_size}å€‹ç‰¹å¾µ")
            
            # é‡å­é›»è·¯
            @qml.qnode(self.dev, interface="torch")
            def quantum_circuit(inputs, weights):
                # æ”¹é€²çš„æ•¸æ“šç·¨ç¢¼ï¼šä½¿ç”¨æ›´å¤šè¼¸å…¥ä¿¡æ¯
                # å°‡inputsåˆ†æˆå…©çµ„ï¼Œåˆ†åˆ¥ç”¨æ–¼è§’åº¦ç·¨ç¢¼å’Œç›¸ä½ç·¨ç¢¼
                n_inputs = len(inputs)
                angle_inputs = inputs[:n_inputs//2]  # å‰åŠéƒ¨åˆ†ç”¨æ–¼è§’åº¦
                phase_inputs = inputs[n_inputs//2:]  # å¾ŒåŠéƒ¨åˆ†ç”¨æ–¼ç›¸ä½
                
                # è§’åº¦ç·¨ç¢¼ï¼šRYé–€ç·¨ç¢¼ä¸»è¦ç‰¹å¾µ
                for i in range(self.n_qubits):
                    qml.RY(angle_inputs[i % len(angle_inputs)], wires=i)
                
                # ç›¸ä½ç·¨ç¢¼ï¼šRZé–€ç·¨ç¢¼æ¬¡è¦ç‰¹å¾µ
                for i in range(self.n_qubits):
                    qml.RZ(phase_inputs[i % len(phase_inputs)], wires=i)
                
                # è®Šåˆ†é‡å­é›»è·¯
                for layer in range(self.n_layers):
                    # å–®é‡å­æ¯”ç‰¹æ—‹è½‰
                    for i in range(self.n_qubits):
                        qml.RX(weights[layer, i, 0], wires=i)
                        qml.RY(weights[layer, i, 1], wires=i)
                        qml.RZ(weights[layer, i, 2], wires=i)
                    
                    # ç³¾çºå±¤ï¼šæ›´è±å¯Œçš„ç³¾çºæ¨¡å¼
                    # ç·šæ€§ç³¾çº
                    for i in range(self.n_qubits - 1):
                        qml.CNOT(wires=[i, i + 1])
                    
                    # ç’°å½¢ç³¾çº (å¦‚æœæœ‰3å€‹ä»¥ä¸Šé‡å­æ¯”ç‰¹)
                    if self.n_qubits > 2:
                        qml.CNOT(wires=[self.n_qubits - 1, 0])
                    
                    # å¦‚æœæ˜¯å¤šå±¤ï¼Œæ·»åŠ é¡å¤–çš„ç³¾çº
                    if layer < self.n_layers - 1 and self.n_qubits >= 4:
                        # äº¤å‰ç³¾çº
                        qml.CNOT(wires=[0, 2])
                        qml.CNOT(wires=[1, 3])
                
                # æ¸¬é‡æ‰€æœ‰é‡å­æ¯”ç‰¹
                return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]
            
            self.quantum_circuit = quantum_circuit
            
            # é‡å­æ¬Šé‡åƒæ•¸
            self.q_weights = nn.Parameter(
                torch.randn(n_layers, n_qubits, 3) * 0.1
            )
            
            # æ”¹é€²çš„ç¶“å…¸å¾Œè™•ç†å±¤
            self.post_net = nn.Sequential(
                nn.Linear(n_qubits, n_qubits * 2),  # å…ˆæ“´å±•
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(n_qubits * 2, n_qubits),  # å†æ”¶ç¸®
                nn.ReLU(),
                nn.Linear(n_qubits, 1)  # æœ€çµ‚è¼¸å‡º
            )
            
        def forward(self, x):
            batch_size = x.size(0)
            
            # ç¢ºä¿è¼¸å…¥æ˜¯Float32é¡å‹
            x = x.float()
            
            # å¦‚æœæ˜¯åºåˆ—æ•¸æ“šï¼Œå–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥
            if len(x.shape) == 3:
                x = x[:, -1, :]  # (batch_size, seq_len, features) -> (batch_size, features)
            
            # ç›´æ¥ä½¿ç”¨PCAé™ç¶­å¾Œçš„ç‰¹å¾µï¼Œç„¡éœ€é¡å¤–é è™•ç†
            pca_features = x  # xå·²ç¶“æ˜¯PCAé™ç¶­å¾Œçš„ç‰¹å¾µ
            
            # é‡å­é›»è·¯è™•ç†
            quantum_results = []
            for i in range(batch_size):
                # é‡å¡‘æ¬Šé‡åƒæ•¸ç”¨æ–¼é‡å­é›»è·¯ï¼Œç¢ºä¿Float32é¡å‹
                weights = self.q_weights.reshape(self.n_layers, self.n_qubits, 3).float()
                
                # é‡å­é›»è·¯å‰å‘å‚³æ’­ï¼Œç›´æ¥ä½¿ç”¨PCAç‰¹å¾µ
                q_out = self.quantum_circuit(pca_features[i].float(), weights)
                
                # å°‡é‡å­é›»è·¯è¼¸å‡ºè½‰æ›ç‚ºPyTorchå¼µé‡ä¸¦ç¢ºä¿Float32
                if isinstance(q_out, (list, tuple)):
                    q_out_tensor = torch.stack([torch.as_tensor(val, dtype=torch.float32) for val in q_out])
                else:
                    q_out_tensor = torch.as_tensor(q_out, dtype=torch.float32)
                
                quantum_results.append(q_out_tensor)
            
            quantum_out = torch.stack(quantum_results).float()
            
            # ç¶“å…¸å¾Œè™•ç†
            output = self.post_net(quantum_out)
            
            return output

    class PyTorchQSVM_QNN(nn.Module):
        """QSVM-QNNæ··åˆæ¨¡å‹ï¼šé‡å­æ”¯æŒå‘é‡æ©Ÿ + é‡å­ç¥ç¶“ç¶²çµ¡"""
        
        def __init__(self, input_size=4, n_qubits=4, n_layers=2, qsvm_features=4):
            super(PyTorchQSVM_QNN, self).__init__()
            self.input_size = input_size  # PCAé™ç¶­å¾Œçš„4å€‹ç‰¹å¾µ
            self.n_qubits = n_qubits  
            self.n_layers = n_layers
            self.qsvm_features = qsvm_features  # QSVMè¼¸å‡ºç‰¹å¾µæ•¸ï¼Œç­‰æ–¼input_size
            
            # æª¢æŸ¥PennyLaneæ˜¯å¦å¯ç”¨
            if not QML_AVAILABLE:
                raise ImportError("PennyLane not available for QSVM-QNN")
                
            print(f"ğŸŒŒ åˆå§‹åŒ–QSVM-QNNæ··åˆæ¨¡å‹ï¼š{input_size}å€‹PCAç‰¹å¾µ â†’ QSVM({input_size}é‡å­æ¯”ç‰¹) â†’ QNN({n_qubits}é‡å­æ¯”ç‰¹)")
            
            # QSVMé‡å­è¨­å‚™ï¼ˆä½¿ç”¨input_sizeå€‹é‡å­æ¯”ç‰¹è™•ç†PCAç‰¹å¾µï¼‰
            self.qsvm_dev = qml.device("default.qubit", wires=input_size)
            
            # QNNé‡å­è¨­å‚™  
            self.qnn_dev = qml.device("default.qubit", wires=n_qubits)
            
            # QSVMé‡å­ç‰¹å¾µæ˜ å°„é›»è·¯
            @qml.qnode(self.qsvm_dev, interface="torch")
            def qsvm_feature_map(x):
                # æ•¸æ“šç·¨ç¢¼åˆ°é‡å­æ…‹
                for i in range(len(x)):
                    qml.RY(x[i], wires=i)
                    qml.RZ(x[i]**2, wires=i)  # éç·šæ€§ç‰¹å¾µæ˜ å°„
                
                # ç³¾çºå±¤ä»¥æ•ç²ç‰¹å¾µé—œè¯
                for i in range(len(x)-1):
                    qml.CNOT(wires=[i, i+1])
                if len(x) > 2:
                    qml.CNOT(wires=[len(x)-1, 0])
                    
                # æ¸¬é‡æ‰€æœ‰é‡å­æ¯”ç‰¹
                return [qml.expval(qml.PauliZ(i)) for i in range(len(x))]
            
            self.qsvm_feature_map = qsvm_feature_map
            
            # QNNéƒ¨åˆ†ï¼ˆæ¥æ”¶QSVMè™•ç†å¾Œçš„ç‰¹å¾µï¼‰
            @qml.qnode(self.qnn_dev, interface="torch") 
            def qnn_circuit(qsvm_features, weights):
                # å°‡QSVMç‰¹å¾µç·¨ç¢¼åˆ°QNN
                for i in range(self.n_qubits):
                    qml.RY(qsvm_features[i % len(qsvm_features)], wires=i)
                
                # è®Šåˆ†é‡å­é›»è·¯
                for layer in range(self.n_layers):
                    # æ—‹è½‰é–€
                    for i in range(self.n_qubits):
                        qml.RX(weights[layer, i, 0], wires=i)
                        qml.RY(weights[layer, i, 1], wires=i)
                        qml.RZ(weights[layer, i, 2], wires=i)
                    
                    # ç³¾çºå±¤
                    for i in range(self.n_qubits - 1):
                        qml.CNOT(wires=[i, i + 1])
                    if self.n_qubits > 2:
                        qml.CNOT(wires=[self.n_qubits - 1, 0])
                
                # æ¸¬é‡
                return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]
            
            self.qnn_circuit = qnn_circuit
            
            # QNNå¯è¨“ç·´åƒæ•¸
            self.qnn_weights = nn.Parameter(
                torch.randn(n_layers, n_qubits, 3) * 0.1
            )
            
            # æœ€çµ‚è¼¸å‡ºå±¤
            self.output_net = nn.Sequential(
                nn.Linear(n_qubits, n_qubits * 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(n_qubits * 2, 1)
            )
            
        def forward(self, x):
            batch_size = x.size(0)
            
            # ç¢ºä¿è¼¸å…¥æ˜¯Float32é¡å‹
            x = x.float()
            
            # å¦‚æœæ˜¯åºåˆ—æ•¸æ“šï¼Œå–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥
            if len(x.shape) == 3:
                x = x[:, -1, :]  # (batch_size, seq_len, features) -> (batch_size, features)
            
            # ç¬¬ä¸€éšæ®µï¼šQSVMç‰¹å¾µæ˜ å°„
            qsvm_results = []
            for i in range(batch_size):
                # QSVMè™•ç†
                qsvm_out = self.qsvm_feature_map(x[i])
                if isinstance(qsvm_out, (list, tuple)):
                    qsvm_tensor = torch.stack([torch.as_tensor(val, dtype=torch.float32) for val in qsvm_out])
                else:
                    qsvm_tensor = torch.as_tensor(qsvm_out, dtype=torch.float32)
                qsvm_results.append(qsvm_tensor)
            
            qsvm_features = torch.stack(qsvm_results).float()
            
            # ç¬¬äºŒéšæ®µï¼šQNNè™•ç†QSVMè¼¸å‡º
            qnn_results = []
            for i in range(batch_size):
                weights = self.qnn_weights.reshape(self.n_layers, self.n_qubits, 3).float()
                qnn_out = self.qnn_circuit(qsvm_features[i], weights)
                
                if isinstance(qnn_out, (list, tuple)):
                    qnn_tensor = torch.stack([torch.as_tensor(val, dtype=torch.float32) for val in qnn_out])
                else:
                    qnn_tensor = torch.as_tensor(qnn_out, dtype=torch.float32)
                qnn_results.append(qnn_tensor)
            
            qnn_output = torch.stack(qnn_results).float()
            
            # æœ€çµ‚è¼¸å‡º
            output = self.output_net(qnn_output)
            
            return output

else:
    # å¦‚æœPyTorchä¸å¯ç”¨ï¼Œå‰µå»ºç©ºçš„æ›¿ä»£é¡
    class PyTorchLSTM:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch not available")
    
    class PyTorchTransformer:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch not available")
    
    class PyTorchQNN:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch or PennyLane not available")
            
    class PyTorchQSVM_QNN:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch or PennyLane not available")

class PyTorchModelComparison:
    """ç´”PyTorchæ¨¡å‹æ¯”è¼ƒç³»çµ±"""
    
    def __init__(self, pool_name='3pool', sequence_length=24):
        self.pool_name = pool_name
        self.sequence_length = sequence_length
        self.models = {}
        self.scalers = {}
        self.results = {}
        
        # æ•¸æ“šç›¸é—œ
        self.data = None
        self.processed_data = None
        
        # ç‰¹å¾µç›¸é—œ
        self.feature_columns = []
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
        print(f"ğŸš€ åˆå§‹åŒ–ç´”PyTorchæ¨¡å‹æ¯”è¼ƒç³»çµ± - {pool_name}")
        
    def load_data(self, file_path=None):
        """è¼‰å…¥æ­·å²æ•¸æ“š"""
        
        if file_path is None:
            file_path = f"free_historical_cache/{self.pool_name}_comprehensive_free_historical_365d.csv"
        
        try:
            self.data = pd.read_csv(file_path)
            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
            
            print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(self.data)} æ¢è¨˜éŒ„")
            print(f"ğŸ“… æ™‚é–“ç¯„åœ: {self.data['timestamp'].min()} åˆ° {self.data['timestamp'].max()}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def create_features(self):
        """ç‰¹å¾µå·¥ç¨‹ - å‰µå»ºé æ¸¬ç‰¹å¾µ"""
        
        print("ğŸ”§ é–‹å§‹ç‰¹å¾µå·¥ç¨‹...")
        
        df = self.data.copy()
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # 1. æ»¯å¾Œç‰¹å¾µ (Lag Features)
        for lag in [1, 6, 24, 168]:  # 1å€‹é»ã€6å€‹é»ã€24å€‹é»ã€168å€‹é»
            df[f'virtual_price_lag_{lag}'] = df['virtual_price'].shift(lag)
        
        # 2. ç§»å‹•å¹³å‡ç‰¹å¾µ (Moving Average)
        for window in [24, 168, 672]:  # 6å°æ™‚ã€7å¤©ã€28å¤©
            df[f'virtual_price_ma_{window}'] = df['virtual_price'].rolling(window).mean()
        
        # 3. æ³¢å‹•ç‡ç‰¹å¾µ (Volatility)
        for window in [24, 168]:
            df[f'virtual_price_std_{window}'] = df['virtual_price'].rolling(window).std()
            df[f'virtual_price_cv_{window}'] = df[f'virtual_price_std_{window}'] / df[f'virtual_price_ma_{window}']
        
        # 4. åƒ¹æ ¼è®ŠåŒ–ç‰¹å¾µ (Price Change)
        df['virtual_price_change'] = df['virtual_price'].pct_change()
        df['virtual_price_change_abs'] = df['virtual_price_change'].abs()
        
        # 5. æµå‹•æ€§ç‰¹å¾µ (Liquidity Features)
        df['total_supply_change'] = df['total_supply'].pct_change()
        df['total_supply_ma_24'] = df['total_supply'].rolling(24).mean()
        
        # 6. é¤˜é¡ç‰¹å¾µ (Balance Features)
        token_columns = [col for col in df.columns if col.endswith('_balance')]
        if len(token_columns) >= 2:
            df['balance_ratio'] = df[token_columns[0]] / df[token_columns[1]]
            df['balance_imbalance'] = df[token_columns].std(axis=1) / df[token_columns].mean(axis=1)
        
        # 7. æ™‚é–“ç‰¹å¾µ (Time Features)
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        
        # 8. æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ (Technical Indicators)
        # RSI (ç›¸å°å¼·å¼±æŒ‡æ•¸)
        df['price_change_positive'] = df['virtual_price_change'].apply(lambda x: x if x > 0 else 0)
        df['price_change_negative'] = df['virtual_price_change'].apply(lambda x: -x if x < 0 else 0)
        df['rsi_14'] = 100 - (100 / (1 + df['price_change_positive'].rolling(14).mean() / 
                                         df['price_change_negative'].rolling(14).mean()))
        
        # 9. ç›®æ¨™è®Šæ•¸ (Target Variable)
        df['target_24h'] = df['virtual_price'].shift(-24)  # é æ¸¬24å€‹é»å¾Œçš„åƒ¹æ ¼ (6å°æ™‚å¾Œ)
        df['target_return_24h'] = (df['target_24h'] / df['virtual_price'] - 1) * 100  # æ”¶ç›Šç‡%
        
        # åˆªé™¤ç¼ºå¤±å€¼
        self.processed_data = df.dropna().reset_index(drop=True)
        
        print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆ")
        print(f"ğŸ“Š è™•ç†å¾Œæ•¸æ“š: {len(self.processed_data)} æ¢è¨˜éŒ„")
        
        return self.processed_data
    
    def prepare_data(self):
        """æº–å‚™ä¸åŒæ¨¡å‹çš„è¨“ç·´æ•¸æ“š"""
        
        # é¸æ“‡ç‰¹å¾µæ¬„
        exclude_cols = ['timestamp', 'pool_address', 'pool_name', 'source', 'target_24h', 'target_return_24h', 'virtual_price']
        self.feature_columns = [col for col in self.processed_data.columns if col not in exclude_cols]
        
        X = self.processed_data[self.feature_columns].fillna(0)
        y = self.processed_data['target_return_24h'].fillna(0)
        
        # æ™‚é–“åºåˆ—åˆ†å‰² (å‰80%è¨“ç·´ï¼Œå¾Œ20%æ¸¬è©¦)
        split_index = int(len(X) * 0.8)
        
        self.X_train = X.iloc[:split_index]
        self.X_test = X.iloc[split_index:]
        self.y_train = y.iloc[:split_index] 
        self.y_test = y.iloc[split_index:]
        
        print(f"ğŸ“Š è¨“ç·´é›†: {len(self.X_train)} æ¨£æœ¬")
        print(f"ğŸ“Š æ¸¬è©¦é›†: {len(self.X_test)} æ¨£æœ¬")
        
    def create_sequences_for_pytorch(self, X, y, sequence_length):
        """ç‚ºPyTorchæ¨¡å‹å‰µå»ºåºåˆ—æ•¸æ“š"""
        
        X_seq = []
        y_seq = []
        
        for i in range(sequence_length, len(X)):
            X_seq.append(X.iloc[i-sequence_length:i].values)
            y_seq.append(y.iloc[i])
            
        return np.array(X_seq), np.array(y_seq)
    
    def train_random_forest(self):
        """è¨“ç·´Random Forestæ¨¡å‹"""
        
        print("\nğŸŒ³ è¨“ç·´Random Forestæ¨¡å‹...")
        
        # æ¨™æº–åŒ–æ•¸æ“š
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # è¨“ç·´æ¨¡å‹
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        rf_model.fit(X_train_scaled, self.y_train)
        
        # é æ¸¬
        train_pred = rf_model.predict(X_train_scaled)
        test_pred = rf_model.predict(X_test_scaled)
        
        # å­˜å„²æ¨¡å‹å’Œçµæœ
        self.models['Random Forest'] = rf_model
        self.scalers['Random Forest'] = scaler
        self.results['Random Forest'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(self.y_train, train_pred),
            'test_mae': mean_absolute_error(self.y_test, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(self.y_train, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_test, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(self.y_train)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(self.y_test)) * 100
        }
        
        print("âœ… Random Forestè¨“ç·´å®Œæˆ")
        
    def train_xgboost(self):
        """è¨“ç·´XGBoostæ¨¡å‹"""
        
        if not XGBOOST_AVAILABLE:
            print("âŒ XGBoostä¸å¯ç”¨ï¼Œè·³éXGBoostè¨“ç·´")
            return
            
        print("\nğŸš€ è¨“ç·´XGBoostæ¨¡å‹...")
        
        # æ¨™æº–åŒ–æ•¸æ“š
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # å‰µå»ºXGBoostæ¨¡å‹ (æ–°ç‰ˆæœ¬èªæ³•)
        xgb_model = xgb.XGBRegressor(
            n_estimators=300,           # æ›´å¤šæ¨¹ä»¥æé«˜æ€§èƒ½
            max_depth=6,                # é©ä¸­æ·±åº¦é¿å…éæ“¬åˆ
            learning_rate=0.1,          # å­¸ç¿’ç‡
            subsample=0.8,              # è¡Œæ¡æ¨£æ¯”ä¾‹
            colsample_bytree=0.8,       # ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹
            reg_alpha=0.1,              # L1æ­£å‰‡åŒ–
            reg_lambda=1.0,             # L2æ­£å‰‡åŒ–
            random_state=42,
            n_jobs=-1,                  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            tree_method='hist',         # æ›´å¿«çš„æ¨¹æ§‹å»ºæ–¹æ³•
            objective='reg:squarederror' # å›æ­¸ç›®æ¨™
        )
        
        # è¨“ç·´æ¨¡å‹ (ç°¡åŒ–ç‰ˆæœ¬ï¼Œç„¡æ—©åœ)
        xgb_model.fit(
            X_train_scaled, self.y_train,
            verbose=False              # ä¸é¡¯ç¤ºè©³ç´°è¨“ç·´éç¨‹
        )
        
        # é æ¸¬
        train_pred = xgb_model.predict(X_train_scaled)
        test_pred = xgb_model.predict(X_test_scaled)
        
        # å­˜å„²æ¨¡å‹å’Œçµæœ
        self.models['XGBoost'] = xgb_model
        self.scalers['XGBoost'] = scaler
        self.results['XGBoost'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(self.y_train, train_pred),
            'test_mae': mean_absolute_error(self.y_test, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(self.y_train, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_test, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(self.y_train)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(self.y_test)) * 100
        }
        
        # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§å‰5å
        try:
            feature_importance = xgb_model.feature_importances_
            top_features = np.argsort(feature_importance)[-5:][::-1]
            print(f"ğŸ“Š XGBoostå‰5é‡è¦ç‰¹å¾µ: {[f'ç‰¹å¾µ{i}' for i in top_features]}")
            print(f"ğŸ“ˆ å°æ‡‰é‡è¦æ€§åˆ†æ•¸: {[f'{feature_importance[i]:.3f}' for i in top_features]}")
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾µé‡è¦æ€§é¡¯ç¤ºå¤±æ•—: {e}")
        
        print("âœ… XGBoostè¨“ç·´å®Œæˆ")
        
    def train_pytorch_lstm(self):
        """ä½¿ç”¨PyTorchè¨“ç·´LSTMæ¨¡å‹"""
        
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³éLSTMè¨“ç·´")
            return
            
        print("\nğŸ”„ è¨“ç·´PyTorch LSTMæ¨¡å‹...")
        
        # æº–å‚™åºåˆ—æ•¸æ“š
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # å‰µå»ºåºåˆ—
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_scaled), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_scaled), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡
        X_train_tensor = torch.FloatTensor(X_train_seq)
        y_train_tensor = torch.FloatTensor(y_train_seq).view(-1, 1)
        X_test_tensor = torch.FloatTensor(X_test_seq)
        y_test_tensor = torch.FloatTensor(y_test_seq).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # å‰µå»ºæ¨¡å‹
        input_size = X_train_tensor.shape[2]
        lstm_model = PyTorchLSTM(input_size, hidden_size=64, num_layers=2)
        
        # è¨“ç·´åƒæ•¸
        criterion = nn.MSELoss()
        optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)
        
        # è¨“ç·´æ¨¡å‹
        lstm_model.train()
        for epoch in range(100):
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = lstm_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/100, Loss: {total_loss/len(train_loader):.4f}")
        
        # é æ¸¬
        lstm_model.eval()
        with torch.no_grad():
            train_pred = lstm_model(X_train_tensor).numpy().flatten()
            test_pred = lstm_model(X_test_tensor).numpy().flatten()
        
        # å­˜å„²çµæœ
        self.models['LSTM (PyTorch)'] = lstm_model
        self.scalers['LSTM (PyTorch)'] = scaler
        self.results['LSTM (PyTorch)'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(y_train_seq, train_pred),
            'test_mae': mean_absolute_error(y_test_seq, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
        }
        
        print("âœ… PyTorch LSTMè¨“ç·´å®Œæˆ")
    
    def train_pytorch_transformer(self):
        """ä½¿ç”¨PyTorchè¨“ç·´Transformeræ¨¡å‹"""
        
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³éTransformerè¨“ç·´")
            return
            
        print("\nğŸ¤– è¨“ç·´PyTorch Transformeræ¨¡å‹...")
        
        # æº–å‚™åºåˆ—æ•¸æ“š (èˆ‡LSTMç›¸åŒ)
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # å‰µå»ºåºåˆ—
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_scaled), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_scaled), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡
        X_train_tensor = torch.FloatTensor(X_train_seq)
        y_train_tensor = torch.FloatTensor(y_train_seq).view(-1, 1)
        X_test_tensor = torch.FloatTensor(X_test_seq)
        y_test_tensor = torch.FloatTensor(y_test_seq).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # å‰µå»ºæ¨¡å‹
        input_size = X_train_tensor.shape[2]
        transformer_model = PyTorchTransformer(input_size, d_model=64, nhead=8, num_layers=3)
        
        # è¨“ç·´åƒæ•¸
        criterion = nn.MSELoss()
        optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)
        
        # è¨“ç·´æ¨¡å‹
        transformer_model.train()
        for epoch in range(100):
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = transformer_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/100, Loss: {total_loss/len(train_loader):.4f}")
        
        # é æ¸¬
        transformer_model.eval()
        with torch.no_grad():
            train_pred = transformer_model(X_train_tensor).numpy().flatten()
            test_pred = transformer_model(X_test_tensor).numpy().flatten()
        
        # å­˜å„²çµæœ
        self.models['Transformer (PyTorch)'] = transformer_model
        self.scalers['Transformer (PyTorch)'] = scaler
        self.results['Transformer (PyTorch)'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(y_train_seq, train_pred),
            'test_mae': mean_absolute_error(y_test_seq, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
        }
        
        print("âœ… PyTorch Transformerè¨“ç·´å®Œæˆ")
    
    def train_pytorch_qnn(self):
        """ä½¿ç”¨PyTorch + PennyLaneè¨“ç·´é‡å­ç¥ç¶“ç¶²çµ¡æ¨¡å‹"""
        
        if not TORCH_AVAILABLE or not QML_AVAILABLE:
            print("âŒ PyTorchæˆ–PennyLaneä¸å¯ç”¨ï¼Œè·³éQNNè¨“ç·´")
            return
            
        print("\nğŸŒŒ è¨“ç·´PyTorché‡å­ç¥ç¶“ç¶²çµ¡(QNN)æ¨¡å‹...")
        
        # æº–å‚™æ•¸æ“š - ä½¿ç”¨PCAé™ç¶­æ›¿ä»£ç¥ç¶“ç¶²çµ¡é è™•ç†
        print("ğŸ”§ ä½¿ç”¨PCAé™ç¶­é€²è¡Œç‰¹å¾µé è™•ç†...")
        
        # æ¨™æº–åŒ–åŸå§‹ç‰¹å¾µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # PCAé™ç¶­åˆ°é©åˆ4å€‹é‡å­æ¯”ç‰¹çš„ç‰¹å¾µæ•¸ (8å€‹ç‰¹å¾µç”¨æ–¼é›™é‡ç·¨ç¢¼)
        target_features = 4 * 2  # 4å€‹é‡å­æ¯”ç‰¹ Ã— 2 (è§’åº¦+ç›¸ä½ç·¨ç¢¼)
        pca = PCA(n_components=target_features)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)
        
        # é¡¯ç¤ºPCAä¿¡æ¯
        explained_variance_ratio = pca.explained_variance_ratio_
        total_variance = sum(explained_variance_ratio)
        print(f"ğŸ“Š PCAé™ç¶­: {self.X_train.shape[1]}ç‰¹å¾µ â†’ {target_features}ç‰¹å¾µ")
        print(f"ğŸ“ˆ ä¿ç•™æ–¹å·®æ¯”ä¾‹: {total_variance:.3f} ({total_variance*100:.1f}%)")
        print(f"ğŸ” å‰3å€‹ä¸»æˆåˆ†æ–¹å·®æ¯”ä¾‹: {explained_variance_ratio[:3]}")
        
        # å‰µå»ºåºåˆ—æ•¸æ“šï¼ˆç”¨æ–¼èˆ‡å…¶ä»–æ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_pca), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_pca), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡ï¼Œç¢ºä¿Float32é¡å‹
        X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)
        X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨ - æ¥µå°æ‰¹æ¬¡ä»¥æé«˜é‡å­è¨ˆç®—ç©©å®šæ€§
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # å–®æ¨£æœ¬æ‰¹æ¬¡ï¼Œæœ€å¤§ç©©å®šæ€§
        
        # å‰µå»ºæ”¹é€²çš„QNNæ¨¡å‹ - æ›´å¥½çš„ç‰¹å¾µç·¨ç¢¼
        input_size = X_train_tensor.shape[2]
        qnn_model = PyTorchQNN(input_size, n_qubits=4, n_layers=2)  # å¢åŠ åˆ°4é‡å­æ¯”ç‰¹ï¼Œ2å±¤
        
        # è¨“ç·´åƒæ•¸ (é‡å­æ¨¡å‹ä½¿ç”¨æ›´ä¿å®ˆçš„è¨­ç½®)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(qnn_model.parameters(), lr=0.005)  # èª¿æ•´å­¸ç¿’ç‡
        
        # è¨“ç·´æ¨¡å‹
        qnn_model.train()
        print("ğŸŒŒ è¨“ç·´æ”¹é€²çš„QNN: 4é‡å­æ¯”ç‰¹ + 2å±¤é›»è·¯ï¼Œæ›´å¼·çš„ç‰¹å¾µç·¨ç¢¼èƒ½åŠ›...")
        
        successful_epochs = 0
        for epoch in range(100):  # æ¸›å°‘epochæ•¸é‡
            total_loss = 0
            batch_count = 0
            epoch_success = True
            
            for batch_X, batch_y in train_loader:
                try:
                    optimizer.zero_grad()
                    outputs = qnn_model(batch_X)
                    loss = criterion(outputs, batch_y)
                    
                    # æª¢æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                    if torch.isfinite(loss):
                        loss.backward()
                        optimizer.step()
                        total_loss += loss.item()
                        batch_count += 1
                    else:
                        print(f"âš ï¸ Epoch {epoch+1}: è·³éç„¡æ•ˆloss")
                        continue
                        
                except Exception as e:
                    print(f"âš ï¸ Epoch {epoch+1}: {str(e)[:50]}...")
                    epoch_success = False
                    break
            
            if epoch_success and batch_count > 0:
                successful_epochs += 1
                if (epoch + 1) % 10 == 0:
                    avg_loss = total_loss / batch_count
                    print(f"Epoch {epoch+1}/100, Loss: {avg_loss:.4f}, æˆåŠŸæ‰¹æ¬¡: {batch_count}")
            
            # å¦‚æœé€£çºŒå¤±æ•—ï¼Œæå‰çµ‚æ­¢
            if epoch > 10 and successful_epochs < epoch * 0.3:
                print(f"âš ï¸ é‡å­è¨“ç·´ä¸ç©©å®šï¼Œæå‰çµ‚æ­¢æ–¼Epoch {epoch+1}")
                break
        
        # é æ¸¬
        qnn_model.eval()
        try:
            with torch.no_grad():
                train_pred = qnn_model(X_train_tensor).numpy().flatten()
                test_pred = qnn_model(X_test_tensor).numpy().flatten()
            
            # å­˜å„²çµæœï¼ˆåŒ…å«PCAè®Šæ›å™¨ï¼‰
            self.models['QNN (PyTorch+PennyLane)'] = qnn_model
            self.scalers['QNN (PyTorch+PennyLane)'] = {'scaler': scaler, 'pca': pca}
            self.results['QNN (PyTorch+PennyLane)'] = {
                'train_pred': train_pred,
                'test_pred': test_pred,
                'train_mae': mean_absolute_error(y_train_seq, train_pred),
                'test_mae': mean_absolute_error(y_test_seq, test_pred),
                'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
                'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
                'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
                'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
            }
            
            print("âœ… PyTorché‡å­ç¥ç¶“ç¶²çµ¡è¨“ç·´å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ QNNé æ¸¬éšæ®µå‡ºéŒ¯: {e}")
            print("âš ï¸ QNNæ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šèª¿å„ª")
    
    def train_pytorch_qsvmqnn(self):
        """ä½¿ç”¨PyTorch + PennyLaneè¨“ç·´QSVM-QNNæ··åˆæ¨¡å‹"""
        
        if not TORCH_AVAILABLE or not QML_AVAILABLE:
            print("âŒ PyTorchæˆ–PennyLaneä¸å¯ç”¨ï¼Œè·³éQSVM-QNNè¨“ç·´")
            return
            
        print("\nğŸŒŒ è¨“ç·´PyTorch QSVM-QNNæ··åˆæ¨¡å‹...")
        
        # æº–å‚™æ•¸æ“š - ä½¿ç”¨PCAé™ç¶­æ›¿ä»£ç¥ç¶“ç¶²çµ¡é è™•ç†
        print("ğŸ”§ ä½¿ç”¨PCAé™ç¶­é€²è¡Œç‰¹å¾µé è™•ç†...")
        
        # æ¨™æº–åŒ–åŸå§‹ç‰¹å¾µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # PCAé™ç¶­åˆ°4å€‹ç‰¹å¾µï¼ˆç”¨æˆ¶è¦æ±‚ï¼‰
        target_features = 8  # é™ç¶­åˆ°4å€‹ç‰¹å¾µé€²å…¥QSVM
        pca = PCA(n_components=target_features)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)
        
        # é¡¯ç¤ºPCAä¿¡æ¯
        explained_variance_ratio = pca.explained_variance_ratio_
        total_variance = sum(explained_variance_ratio)
        print(f"ğŸ“Š PCAé™ç¶­: {self.X_train.shape[1]}ç‰¹å¾µ â†’ {target_features}ç‰¹å¾µ (é€²å…¥QSVM)")
        print(f"ğŸ“ˆ ä¿ç•™æ–¹å·®æ¯”ä¾‹: {total_variance:.3f} ({total_variance*100:.1f}%)")
        print(f"ğŸ” ä¸»æˆåˆ†æ–¹å·®æ¯”ä¾‹: {explained_variance_ratio}")
        
        # å‰µå»ºåºåˆ—æ•¸æ“šï¼ˆç”¨æ–¼èˆ‡å…¶ä»–æ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_pca), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_pca), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡ï¼Œç¢ºä¿Float32é¡å‹
        X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)
        X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨ - æ¥µå°æ‰¹æ¬¡ä»¥æé«˜é‡å­è¨ˆç®—ç©©å®šæ€§
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # å–®æ¨£æœ¬æ‰¹æ¬¡ï¼Œæœ€å¤§ç©©å®šæ€§
        
        # å‰µå»ºQSVM-QNNæ··åˆæ¨¡å‹ï¼š4å€‹PCAç‰¹å¾µ â†’ QSVM â†’ QNN
        input_size = X_train_tensor.shape[2]  # æ‡‰è©²æ˜¯4
        qsvmqnn_model = PyTorchQSVM_QNN(input_size=input_size, n_qubits=4, n_layers=2)
        
        # è¨“ç·´åƒæ•¸ (é‡å­æ¨¡å‹ä½¿ç”¨æ›´ä¿å®ˆçš„è¨­ç½®ï¼Œä½¿ç”¨HuberLoss)
        criterion = nn.HuberLoss(delta=1.0)  # HuberLosså°ç•°å¸¸å€¼æ›´é­¯æ£’
        optimizer = optim.Adam(qsvmqnn_model.parameters(), lr=0.005)  # èª¿æ•´å­¸ç¿’ç‡
        
        # è¨“ç·´æ¨¡å‹
        qsvmqnn_model.train()
        print("ğŸŒŒ è¨“ç·´QSVM-QNN: QSVMç‰¹å¾µæ˜ å°„ + QNNå­¸ç¿’ + HuberLossé­¯æ£’è¨“ç·´...")
        
        successful_epochs = 0
        for epoch in range(100):  # æ¸›å°‘epochæ•¸é‡
            total_loss = 0
            batch_count = 0
            epoch_success = True
            
            for batch_X, batch_y in train_loader:
                try:
                    optimizer.zero_grad()
                    outputs = qsvmqnn_model(batch_X)
                    loss = criterion(outputs, batch_y)
                    
                    # æª¢æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                    if torch.isfinite(loss):
                        loss.backward()
                        optimizer.step()
                        total_loss += loss.item()
                        batch_count += 1
                    else:
                        print(f"âš ï¸ Epoch {epoch+1}: è·³éç„¡æ•ˆloss")
                        continue
                        
                except Exception as e:
                    print(f"âš ï¸ Epoch {epoch+1}: {str(e)[:50]}...")
                    epoch_success = False
                    break
            
            if epoch_success and batch_count > 0:
                successful_epochs += 1
                if (epoch + 1) % 10 == 0:
                    avg_loss = total_loss / batch_count
                    print(f"Epoch {epoch+1}/100, Loss: {avg_loss:.4f}, æˆåŠŸæ‰¹æ¬¡: {batch_count}")
            
            # å¦‚æœé€£çºŒå¤±æ•—ï¼Œæå‰çµ‚æ­¢
            if epoch > 10 and successful_epochs < epoch * 0.3:
                print(f"âš ï¸ é‡å­è¨“ç·´ä¸ç©©å®šï¼Œæå‰çµ‚æ­¢æ–¼Epoch {epoch+1}")
                break
        
        # é æ¸¬
        qsvmqnn_model.eval()
        try:
            with torch.no_grad():
                train_pred = qsvmqnn_model(X_train_tensor).numpy().flatten()
                test_pred = qsvmqnn_model(X_test_tensor).numpy().flatten()
            
            # å­˜å„²çµæœï¼ˆåŒ…å«PCAè®Šæ›å™¨ï¼‰
            self.models['QSVM-QNN (PyTorch+PennyLane)'] = qsvmqnn_model
            self.scalers['QSVM-QNN (PyTorch+PennyLane)'] = {'scaler': scaler, 'pca': pca}
            self.results['QSVM-QNN (PyTorch+PennyLane)'] = {
                'train_pred': train_pred,
                'test_pred': test_pred,
                'train_mae': mean_absolute_error(y_train_seq, train_pred),
                'test_mae': mean_absolute_error(y_test_seq, test_pred),
                'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
                'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
                'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
                'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
            }
            
            print("âœ… PyTorch QSVM-QNNè¨“ç·´å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ QSVM-QNNé æ¸¬éšæ®µå‡ºéŒ¯: {e}")
            print("âš ï¸ QSVM-QNNæ¨¡å‹å¯èƒ½éœ€è¦æ›´å¤šèª¿å„ª")
    
    def compare_models(self):
        """æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
        
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 80)
        
        # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
        comparison_data = []
        
        for model_name, results in self.results.items():
            comparison_data.append({
                'Model': model_name,
                'Test MAE': results['test_mae'],
                'Test RMSE': results['test_rmse'],
                'Test Direction Accuracy (%)': results['test_direction_acc'],
                'Train Direction Accuracy (%)': results['train_direction_acc']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('Test Direction Accuracy (%)', ascending=False)
        
        print(comparison_df.round(3).to_string(index=False))
        
        return comparison_df
    
    def visualize_predictions(self, last_n_points=200):
        """å¯è¦–åŒ–æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ"""
        
        if not self.results:
            print("âŒ æ²’æœ‰è¨“ç·´å¥½çš„æ¨¡å‹")
            return
            
        plt.figure(figsize=(15, 10))
        
        # ç‚ºæ¯å€‹æ¨¡å‹å‰µå»ºå­åœ–
        n_models = len(self.results)
        cols = 2
        rows = (n_models + 1) // 2
        
        for idx, (model_name, results) in enumerate(self.results.items()):
            plt.subplot(rows, cols, idx + 1)
            
            # ç²å–æ¸¬è©¦é›†é æ¸¬çµæœ
            test_pred = results['test_pred']
            
            # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´å¯¦éš›å€¼
            if 'PyTorch' in model_name:
                # æ·±åº¦å­¸ç¿’æ¨¡å‹ä½¿ç”¨åºåˆ—æ•¸æ“šï¼Œéœ€è¦èª¿æ•´
                actual_values = self.y_test.iloc[self.sequence_length:].tail(last_n_points).values
                pred_values = test_pred[-last_n_points:]
            else:
                # Random Forestä½¿ç”¨å®Œæ•´æ¸¬è©¦é›†
                actual_values = self.y_test.tail(last_n_points).values
                pred_values = test_pred[-last_n_points:]
            
            # ç¢ºä¿é•·åº¦ä¸€è‡´
            min_length = min(len(actual_values), len(pred_values))
            actual_values = actual_values[-min_length:]
            pred_values = pred_values[-min_length:]
            
            # ç¹ªåˆ¶é æ¸¬ vs å¯¦éš›
            plt.plot(actual_values, label='å¯¦éš›æ”¶ç›Šç‡', alpha=0.7)
            plt.plot(pred_values, label='é æ¸¬æ”¶ç›Šç‡', alpha=0.7)
            
            plt.title(f'{model_name}\næº–ç¢ºç‡: {results["test_direction_acc"]:.1f}%')
            plt.ylabel('æ”¶ç›Šç‡ (%)')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_pytorch_comparison_predictions.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_performance_comparison(self):
        """ç¹ªåˆ¶æ€§èƒ½æ¯”è¼ƒåœ–è¡¨"""
        
        if not self.results:
            print("âŒ æ²’æœ‰è¨“ç·´å¥½çš„æ¨¡å‹")
            return
            
        # æº–å‚™æ•¸æ“š
        model_names = list(self.results.keys())
        test_accuracies = [self.results[name]['test_direction_acc'] for name in model_names]
        test_maes = [self.results[name]['test_mae'] for name in model_names]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æº–ç¢ºç‡æ¯”è¼ƒ
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum'][:len(model_names)]
        bars1 = ax1.bar(model_names, test_accuracies, color=colors)
        ax1.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='éš¨æ©Ÿæ°´æº–(50%)')
        ax1.set_title('ğŸ“Š æ¨¡å‹æ–¹å‘æº–ç¢ºç‡æ¯”è¼ƒ (å…¨PyTorch)', fontsize=14, fontweight='bold')
        ax1.set_ylabel('æº–ç¢ºç‡ (%)')
        ax1.set_ylim(40, max(test_accuracies) + 5)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, acc in zip(bars1, test_accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        # MAEæ¯”è¼ƒ
        bars2 = ax2.bar(model_names, test_maes, color=colors)
        ax2.set_title('ğŸ“ˆ æ¨¡å‹å¹³å‡çµ•å°èª¤å·®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
        ax2.set_ylabel('MAE (%)')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, mae in zip(bars2, test_maes):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{mae:.3f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_pytorch_performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_complete_comparison(self):
        """é‹è¡Œå®Œæ•´çš„æ¨¡å‹æ¯”è¼ƒ"""
        
        print("ğŸš€ é–‹å§‹å®Œæ•´çš„PyTorchæ¨¡å‹æ¯”è¼ƒå¯¦é©—...")
        print("=" * 80)
        
        # 1. è¼‰å…¥å’Œæº–å‚™æ•¸æ“š
        if not self.load_data():
            return
        
        self.create_features()
        self.prepare_data()
        
        # 2. è¨“ç·´æ‰€æœ‰å¯ç”¨æ¨¡å‹
        self.train_random_forest()
        self.train_xgboost()  # æ–°å¢XGBoostè¨“ç·´
        
        if TORCH_AVAILABLE:
            self.train_pytorch_lstm()
            self.train_pytorch_transformer()
            
            # è¨“ç·´é‡å­ç¥ç¶“ç¶²çµ¡ (å¦‚æœPennyLaneå¯ç”¨)
            if QML_AVAILABLE:
                self.train_pytorch_qnn()
                self.train_pytorch_qsvmqnn() # æ–°å¢QSVM-QNNè¨“ç·´
            else:
                print("âš ï¸ è·³éQNNè¨“ç·´ï¼Œéœ€è¦å®‰è£PennyLane: pip install pennylane")
        
        # 3. æ¯”è¼ƒæ¨¡å‹æ€§èƒ½
        comparison_df = self.compare_models()
        
        # 4. å¯è¦–åŒ–çµæœ
        self.visualize_predictions()
        self.plot_performance_comparison()
        
        # 5. ç”Ÿæˆå ±å‘Š
        self.generate_comparison_report(comparison_df)
        
        print("\nğŸ‰ PyTorchæ¨¡å‹æ¯”è¼ƒå¯¦é©—å®Œæˆï¼")
        return comparison_df
    
    def generate_comparison_report(self, comparison_df):
        """ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒå ±å‘Š"""
        
        report = []
        report.append(f"# ğŸš€ {self.pool_name} æ± å­ç´”PyTorchæ¨¡å‹æ¯”è¼ƒå ±å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ™‚é–“: {pd.Timestamp.now()}")
        report.append(f"æ¡†æ¶: ç´”PyTorchå¯¦ç¾ (Random Forest + PyTorch LSTM + PyTorch Transformer)")
        report.append(f"æ•¸æ“šæœŸé–“: 365å¤©")
        report.append(f"é æ¸¬ç›®æ¨™: æœªä¾†6å°æ™‚Virtual Priceæ”¶ç›Šç‡")
        report.append("")
        
        # æœ€ä½³æ¨¡å‹
        if len(comparison_df) > 0:
            best_model = comparison_df.iloc[0]
            report.append("ğŸ† æœ€ä½³è¡¨ç¾æ¨¡å‹:")
            report.append(f"æ¨¡å‹: {best_model['Model']}")
            report.append(f"æ¸¬è©¦æº–ç¢ºç‡: {best_model['Test Direction Accuracy (%)']:.2f}%")
            report.append(f"æ¸¬è©¦MAE: {best_model['Test MAE']:.4f}%")
            report.append("")
            
            # æ¨¡å‹æ’å
            report.append("ğŸ“Š æ¨¡å‹æ’å (æŒ‰æº–ç¢ºç‡):")
            for idx, row in comparison_df.iterrows():
                rank = idx + 1
                model = row['Model']
                acc = row['Test Direction Accuracy (%)']
                mae = row['Test MAE']
                report.append(f"{rank}. {model}: {acc:.2f}% (MAE: {mae:.4f})")
            
            report.append("")
            report.append("ğŸ’¡ å»ºè­°:")
            
            if best_model['Test Direction Accuracy (%)'] > 70:
                report.append("âœ… æœ€ä½³æ¨¡å‹è¡¨ç¾å„ªç§€ï¼Œå»ºè­°ç”¨æ–¼å¯¦éš›é æ¸¬")
            elif best_model['Test Direction Accuracy (%)'] > 60:
                report.append("âš–ï¸ æœ€ä½³æ¨¡å‹è¡¨ç¾ä¸­ç­‰ï¼Œå»ºè­°è¬¹æ…ä½¿ç”¨")
            else:
                report.append("âš ï¸ æ‰€æœ‰æ¨¡å‹è¡¨ç¾ä¸€èˆ¬ï¼Œå»ºè­°æ”¹é€²ç‰¹å¾µå·¥ç¨‹")
        
        # ä¿å­˜å ±å‘Š
        report_content = "\n".join(report)
        with open(f'{self.pool_name}_pytorch_comparison_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜: {self.pool_name}_pytorch_comparison_report.txt")

def demo_pytorch_model_comparison():
    """æ¼”ç¤ºç´”PyTorchæ¨¡å‹æ¯”è¼ƒ"""
    
    print("ğŸš€ Curve Virtual Priceé æ¸¬ - ç´”PyTorchæ¨¡å‹æ¯”è¼ƒæ¼”ç¤º")
    print("=" * 80)
    
    # å‰µå»ºæ¯”è¼ƒå™¨
    comparator = PyTorchModelComparison(pool_name='3pool', sequence_length=24)
    
    # é‹è¡Œå®Œæ•´æ¯”è¼ƒ
    comparison_df = comparator.run_complete_comparison()
    
    if comparison_df is not None:
        print(f"\nğŸ¯ å¯¦é©—ç¸½çµ:")
        print(f"æœ€ä½³æ¨¡å‹: {comparison_df.iloc[0]['Model']}")
        print(f"æœ€é«˜æº–ç¢ºç‡: {comparison_df.iloc[0]['Test Direction Accuracy (%)']:.2f}%")
        print(f"æ¡†æ¶: ç´”PyTorchå¯¦ç¾ (çµ±ä¸€æ¡†æ¶)")
        
        # ä¿å­˜æ¯”è¼ƒçµæœ
        comparison_df.to_csv(f'3pool_pytorch_comparison_results.csv', index=False)
        print("ğŸ’¾ æ¯”è¼ƒçµæœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶")

if __name__ == "__main__":
    demo_pytorch_model_comparison() 