#!/usr/bin/env python3
"""
ğŸš€ ç´”PyTorchæ¨¡å‹æ¯”è¼ƒç³»çµ±
Random Forest vs LSTM vs Transformer - å…¨éƒ¨ä½¿ç”¨PyTorchå¯¦ç¾
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import warnings
warnings.filterwarnings('ignore')

# PyTorchç›¸é—œå°å…¥
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
    print("âœ… PyTorchå¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorchä¸å¯ç”¨ï¼Œå°‡åªä½¿ç”¨Random Forest")
    # å‰µå»ºç©ºçš„æ›¿ä»£é¡
    class nn:
        class Module: pass
        class LSTM: pass
        class Linear: pass
        class Dropout: pass
        class TransformerEncoderLayer: pass
        class TransformerEncoder: pass
        class MSELoss: pass
        class Parameter: pass

# PyTorchæ¨¡å‹å®šç¾©
if TORCH_AVAILABLE:
    class PyTorchLSTM(nn.Module):
        """PyTorch LSTMé æ¸¬æ¨¡å‹"""
        
        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
            super(PyTorchLSTM, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            
            self.lstm = nn.LSTM(
                input_size=input_size, 
                hidden_size=hidden_size, 
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True
            )
            self.dropout = nn.Dropout(dropout)
            self.fc = nn.Linear(hidden_size, 1)
            
        def forward(self, x):
            # LSTMå±¤
            lstm_out, _ = self.lstm(x)
            
            # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
            last_output = lstm_out[:, -1, :]
            
            # Dropoutå’Œå…¨é€£æ¥å±¤
            output = self.dropout(last_output)
            output = self.fc(output)
            
            return output

    class PyTorchTransformer(nn.Module):
        """PyTorch Transformeré æ¸¬æ¨¡å‹"""
        
        def __init__(self, input_size, d_model=64, nhead=8, num_layers=3, dropout=0.1):
            super(PyTorchTransformer, self).__init__()
            self.d_model = d_model
            
            # è¼¸å…¥æŠ•å½±
            self.input_projection = nn.Linear(input_size, d_model)
            
            # ä½ç½®ç·¨ç¢¼
            self.pos_embedding = nn.Parameter(torch.randn(1000, d_model))
            
            # Transformer Encoder
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_model * 4,
                dropout=dropout,
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
            
            # è¼¸å‡ºå±¤
            self.dropout = nn.Dropout(dropout)
            self.fc = nn.Linear(d_model, 1)
            
        def forward(self, x):
            # è¼¸å…¥æŠ•å½±
            x = self.input_projection(x)
            
            # æ·»åŠ ä½ç½®ç·¨ç¢¼
            seq_len = x.size(1)
            pos_emb = self.pos_embedding[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)
            x = x + pos_emb
            
            # Transformerè™•ç†
            x = self.transformer(x)
            
            # å…¨å±€å¹³å‡æ± åŒ–
            x = x.mean(dim=1)
            
            # è¼¸å‡ºå±¤
            x = self.dropout(x)
            x = self.fc(x)
            
            return x

else:
    # å¦‚æœPyTorchä¸å¯ç”¨ï¼Œå‰µå»ºç©ºçš„æ›¿ä»£é¡
    class PyTorchLSTM:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch not available")
    
    class PyTorchTransformer:
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch not available")

class PyTorchModelComparison:
    """ç´”PyTorchæ¨¡å‹æ¯”è¼ƒç³»çµ±"""
    
    def __init__(self, pool_name='3pool', sequence_length=24):
        self.pool_name = pool_name
        self.sequence_length = sequence_length
        self.models = {}
        self.scalers = {}
        self.results = {}
        
        # æ•¸æ“šç›¸é—œ
        self.data = None
        self.processed_data = None
        
        # ç‰¹å¾µç›¸é—œ
        self.feature_columns = []
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
        print(f"ğŸš€ åˆå§‹åŒ–ç´”PyTorchæ¨¡å‹æ¯”è¼ƒç³»çµ± - {pool_name}")
        
    def load_data(self, file_path=None):
        """è¼‰å…¥æ­·å²æ•¸æ“š"""
        
        if file_path is None:
            file_path = f"free_historical_cache/{self.pool_name}_comprehensive_free_historical_365d.csv"
        
        try:
            self.data = pd.read_csv(file_path)
            self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
            
            print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(self.data)} æ¢è¨˜éŒ„")
            print(f"ğŸ“… æ™‚é–“ç¯„åœ: {self.data['timestamp'].min()} åˆ° {self.data['timestamp'].max()}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def create_features(self):
        """ç‰¹å¾µå·¥ç¨‹ - å‰µå»ºé æ¸¬ç‰¹å¾µ"""
        
        print("ğŸ”§ é–‹å§‹ç‰¹å¾µå·¥ç¨‹...")
        
        df = self.data.copy()
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # 1. æ»¯å¾Œç‰¹å¾µ (Lag Features)
        for lag in [1, 6, 24, 168]:  # 1å€‹é»ã€6å€‹é»ã€24å€‹é»ã€168å€‹é»
            df[f'virtual_price_lag_{lag}'] = df['virtual_price'].shift(lag)
        
        # 2. ç§»å‹•å¹³å‡ç‰¹å¾µ (Moving Average)
        for window in [24, 168, 672]:  # 6å°æ™‚ã€7å¤©ã€28å¤©
            df[f'virtual_price_ma_{window}'] = df['virtual_price'].rolling(window).mean()
        
        # 3. æ³¢å‹•ç‡ç‰¹å¾µ (Volatility)
        for window in [24, 168]:
            df[f'virtual_price_std_{window}'] = df['virtual_price'].rolling(window).std()
            df[f'virtual_price_cv_{window}'] = df[f'virtual_price_std_{window}'] / df[f'virtual_price_ma_{window}']
        
        # 4. åƒ¹æ ¼è®ŠåŒ–ç‰¹å¾µ (Price Change)
        df['virtual_price_change'] = df['virtual_price'].pct_change()
        df['virtual_price_change_abs'] = df['virtual_price_change'].abs()
        
        # 5. æµå‹•æ€§ç‰¹å¾µ (Liquidity Features)
        df['total_supply_change'] = df['total_supply'].pct_change()
        df['total_supply_ma_24'] = df['total_supply'].rolling(24).mean()
        
        # 6. é¤˜é¡ç‰¹å¾µ (Balance Features)
        token_columns = [col for col in df.columns if col.endswith('_balance')]
        if len(token_columns) >= 2:
            df['balance_ratio'] = df[token_columns[0]] / df[token_columns[1]]
            df['balance_imbalance'] = df[token_columns].std(axis=1) / df[token_columns].mean(axis=1)
        
        # 7. æ™‚é–“ç‰¹å¾µ (Time Features)
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        
        # 8. æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ (Technical Indicators)
        # RSI (ç›¸å°å¼·å¼±æŒ‡æ•¸)
        df['price_change_positive'] = df['virtual_price_change'].apply(lambda x: x if x > 0 else 0)
        df['price_change_negative'] = df['virtual_price_change'].apply(lambda x: -x if x < 0 else 0)
        df['rsi_14'] = 100 - (100 / (1 + df['price_change_positive'].rolling(14).mean() / 
                                         df['price_change_negative'].rolling(14).mean()))
        
        # 9. ç›®æ¨™è®Šæ•¸ (Target Variable)
        df['target_24h'] = df['virtual_price'].shift(-24)  # é æ¸¬24å€‹é»å¾Œçš„åƒ¹æ ¼ (6å°æ™‚å¾Œ)
        df['target_return_24h'] = (df['target_24h'] / df['virtual_price'] - 1) * 100  # æ”¶ç›Šç‡%
        
        # åˆªé™¤ç¼ºå¤±å€¼
        self.processed_data = df.dropna().reset_index(drop=True)
        
        print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆ")
        print(f"ğŸ“Š è™•ç†å¾Œæ•¸æ“š: {len(self.processed_data)} æ¢è¨˜éŒ„")
        
        return self.processed_data
    
    def prepare_data(self):
        """æº–å‚™ä¸åŒæ¨¡å‹çš„è¨“ç·´æ•¸æ“š"""
        
        # é¸æ“‡ç‰¹å¾µæ¬„
        exclude_cols = ['timestamp', 'pool_address', 'pool_name', 'source', 'target_24h', 'target_return_24h', 'virtual_price']
        self.feature_columns = [col for col in self.processed_data.columns if col not in exclude_cols]
        
        X = self.processed_data[self.feature_columns].fillna(0)
        y = self.processed_data['target_return_24h'].fillna(0)
        
        # æ™‚é–“åºåˆ—åˆ†å‰² (å‰80%è¨“ç·´ï¼Œå¾Œ20%æ¸¬è©¦)
        split_index = int(len(X) * 0.8)
        
        self.X_train = X.iloc[:split_index]
        self.X_test = X.iloc[split_index:]
        self.y_train = y.iloc[:split_index] 
        self.y_test = y.iloc[split_index:]
        
        print(f"ğŸ“Š è¨“ç·´é›†: {len(self.X_train)} æ¨£æœ¬")
        print(f"ğŸ“Š æ¸¬è©¦é›†: {len(self.X_test)} æ¨£æœ¬")
        
    def create_sequences_for_pytorch(self, X, y, sequence_length):
        """ç‚ºPyTorchæ¨¡å‹å‰µå»ºåºåˆ—æ•¸æ“š"""
        
        X_seq = []
        y_seq = []
        
        for i in range(sequence_length, len(X)):
            X_seq.append(X.iloc[i-sequence_length:i].values)
            y_seq.append(y.iloc[i])
            
        return np.array(X_seq), np.array(y_seq)
    
    def train_random_forest(self):
        """è¨“ç·´Random Forestæ¨¡å‹"""
        
        print("\nğŸŒ³ è¨“ç·´Random Forestæ¨¡å‹...")
        
        # æ¨™æº–åŒ–æ•¸æ“š
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # è¨“ç·´æ¨¡å‹
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        rf_model.fit(X_train_scaled, self.y_train)
        
        # é æ¸¬
        train_pred = rf_model.predict(X_train_scaled)
        test_pred = rf_model.predict(X_test_scaled)
        
        # å­˜å„²æ¨¡å‹å’Œçµæœ
        self.models['Random Forest'] = rf_model
        self.scalers['Random Forest'] = scaler
        self.results['Random Forest'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(self.y_train, train_pred),
            'test_mae': mean_absolute_error(self.y_test, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(self.y_train, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_test, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(self.y_train)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(self.y_test)) * 100
        }
        
        print("âœ… Random Forestè¨“ç·´å®Œæˆ")
        
    def train_pytorch_lstm(self):
        """ä½¿ç”¨PyTorchè¨“ç·´LSTMæ¨¡å‹"""
        
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³éLSTMè¨“ç·´")
            return
            
        print("\nğŸ”„ è¨“ç·´PyTorch LSTMæ¨¡å‹...")
        
        # æº–å‚™åºåˆ—æ•¸æ“š
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # å‰µå»ºåºåˆ—
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_scaled), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_scaled), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡
        X_train_tensor = torch.FloatTensor(X_train_seq)
        y_train_tensor = torch.FloatTensor(y_train_seq).view(-1, 1)
        X_test_tensor = torch.FloatTensor(X_test_seq)
        y_test_tensor = torch.FloatTensor(y_test_seq).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # å‰µå»ºæ¨¡å‹
        input_size = X_train_tensor.shape[2]
        lstm_model = PyTorchLSTM(input_size, hidden_size=64, num_layers=2)
        
        # è¨“ç·´åƒæ•¸
        criterion = nn.MSELoss()
        optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)
        
        # è¨“ç·´æ¨¡å‹
        lstm_model.train()
        for epoch in range(100):
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = lstm_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/100, Loss: {total_loss/len(train_loader):.4f}")
        
        # é æ¸¬
        lstm_model.eval()
        with torch.no_grad():
            train_pred = lstm_model(X_train_tensor).numpy().flatten()
            test_pred = lstm_model(X_test_tensor).numpy().flatten()
        
        # å­˜å„²çµæœ
        self.models['LSTM (PyTorch)'] = lstm_model
        self.scalers['LSTM (PyTorch)'] = scaler
        self.results['LSTM (PyTorch)'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(y_train_seq, train_pred),
            'test_mae': mean_absolute_error(y_test_seq, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
        }
        
        print("âœ… PyTorch LSTMè¨“ç·´å®Œæˆ")
    
    def train_pytorch_transformer(self):
        """ä½¿ç”¨PyTorchè¨“ç·´Transformeræ¨¡å‹"""
        
        if not TORCH_AVAILABLE:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³éTransformerè¨“ç·´")
            return
            
        print("\nğŸ¤– è¨“ç·´PyTorch Transformeræ¨¡å‹...")
        
        # æº–å‚™åºåˆ—æ•¸æ“š (èˆ‡LSTMç›¸åŒ)
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # å‰µå»ºåºåˆ—
        X_train_seq, y_train_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_train_scaled), self.y_train, self.sequence_length
        )
        X_test_seq, y_test_seq = self.create_sequences_for_pytorch(
            pd.DataFrame(X_test_scaled), self.y_test, self.sequence_length
        )
        
        # è½‰æ›ç‚ºPyTorchå¼µé‡
        X_train_tensor = torch.FloatTensor(X_train_seq)
        y_train_tensor = torch.FloatTensor(y_train_seq).view(-1, 1)
        X_test_tensor = torch.FloatTensor(X_test_seq)
        y_test_tensor = torch.FloatTensor(y_test_seq).view(-1, 1)
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # å‰µå»ºæ¨¡å‹
        input_size = X_train_tensor.shape[2]
        transformer_model = PyTorchTransformer(input_size, d_model=64, nhead=8, num_layers=3)
        
        # è¨“ç·´åƒæ•¸
        criterion = nn.MSELoss()
        optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)
        
        # è¨“ç·´æ¨¡å‹
        transformer_model.train()
        for epoch in range(100):
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = transformer_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/100, Loss: {total_loss/len(train_loader):.4f}")
        
        # é æ¸¬
        transformer_model.eval()
        with torch.no_grad():
            train_pred = transformer_model(X_train_tensor).numpy().flatten()
            test_pred = transformer_model(X_test_tensor).numpy().flatten()
        
        # å­˜å„²çµæœ
        self.models['Transformer (PyTorch)'] = transformer_model
        self.scalers['Transformer (PyTorch)'] = scaler
        self.results['Transformer (PyTorch)'] = {
            'train_pred': train_pred,
            'test_pred': test_pred,
            'train_mae': mean_absolute_error(y_train_seq, train_pred),
            'test_mae': mean_absolute_error(y_test_seq, test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train_seq, train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_seq, test_pred)),
            'train_direction_acc': np.mean(np.sign(train_pred) == np.sign(y_train_seq)) * 100,
            'test_direction_acc': np.mean(np.sign(test_pred) == np.sign(y_test_seq)) * 100
        }
        
        print("âœ… PyTorch Transformerè¨“ç·´å®Œæˆ")
    
    def compare_models(self):
        """æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
        
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 80)
        
        # å‰µå»ºæ¯”è¼ƒè¡¨æ ¼
        comparison_data = []
        
        for model_name, results in self.results.items():
            comparison_data.append({
                'Model': model_name,
                'Test MAE': results['test_mae'],
                'Test RMSE': results['test_rmse'],
                'Test Direction Accuracy (%)': results['test_direction_acc'],
                'Train Direction Accuracy (%)': results['train_direction_acc']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('Test Direction Accuracy (%)', ascending=False)
        
        print(comparison_df.round(3).to_string(index=False))
        
        return comparison_df
    
    def visualize_predictions(self, last_n_points=200):
        """å¯è¦–åŒ–æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ"""
        
        if not self.results:
            print("âŒ æ²’æœ‰è¨“ç·´å¥½çš„æ¨¡å‹")
            return
            
        plt.figure(figsize=(15, 10))
        
        # ç‚ºæ¯å€‹æ¨¡å‹å‰µå»ºå­åœ–
        n_models = len(self.results)
        cols = 2
        rows = (n_models + 1) // 2
        
        for idx, (model_name, results) in enumerate(self.results.items()):
            plt.subplot(rows, cols, idx + 1)
            
            # ç²å–æ¸¬è©¦é›†é æ¸¬çµæœ
            test_pred = results['test_pred']
            
            # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´å¯¦éš›å€¼
            if 'PyTorch' in model_name:
                # æ·±åº¦å­¸ç¿’æ¨¡å‹ä½¿ç”¨åºåˆ—æ•¸æ“šï¼Œéœ€è¦èª¿æ•´
                actual_values = self.y_test.iloc[self.sequence_length:].tail(last_n_points).values
                pred_values = test_pred[-last_n_points:]
            else:
                # Random Forestä½¿ç”¨å®Œæ•´æ¸¬è©¦é›†
                actual_values = self.y_test.tail(last_n_points).values
                pred_values = test_pred[-last_n_points:]
            
            # ç¢ºä¿é•·åº¦ä¸€è‡´
            min_length = min(len(actual_values), len(pred_values))
            actual_values = actual_values[-min_length:]
            pred_values = pred_values[-min_length:]
            
            # ç¹ªåˆ¶é æ¸¬ vs å¯¦éš›
            plt.plot(actual_values, label='å¯¦éš›æ”¶ç›Šç‡', alpha=0.7)
            plt.plot(pred_values, label='é æ¸¬æ”¶ç›Šç‡', alpha=0.7)
            
            plt.title(f'{model_name}\næº–ç¢ºç‡: {results["test_direction_acc"]:.1f}%')
            plt.ylabel('æ”¶ç›Šç‡ (%)')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_pytorch_comparison_predictions.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_performance_comparison(self):
        """ç¹ªåˆ¶æ€§èƒ½æ¯”è¼ƒåœ–è¡¨"""
        
        if not self.results:
            print("âŒ æ²’æœ‰è¨“ç·´å¥½çš„æ¨¡å‹")
            return
            
        # æº–å‚™æ•¸æ“š
        model_names = list(self.results.keys())
        test_accuracies = [self.results[name]['test_direction_acc'] for name in model_names]
        test_maes = [self.results[name]['test_mae'] for name in model_names]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æº–ç¢ºç‡æ¯”è¼ƒ
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum'][:len(model_names)]
        bars1 = ax1.bar(model_names, test_accuracies, color=colors)
        ax1.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='éš¨æ©Ÿæ°´æº–(50%)')
        ax1.set_title('ğŸ“Š æ¨¡å‹æ–¹å‘æº–ç¢ºç‡æ¯”è¼ƒ (å…¨PyTorch)', fontsize=14, fontweight='bold')
        ax1.set_ylabel('æº–ç¢ºç‡ (%)')
        ax1.set_ylim(40, max(test_accuracies) + 5)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, acc in zip(bars1, test_accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        # MAEæ¯”è¼ƒ
        bars2 = ax2.bar(model_names, test_maes, color=colors)
        ax2.set_title('ğŸ“ˆ æ¨¡å‹å¹³å‡çµ•å°èª¤å·®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
        ax2.set_ylabel('MAE (%)')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, mae in zip(bars2, test_maes):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{mae:.3f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'{self.pool_name}_pytorch_performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_complete_comparison(self):
        """é‹è¡Œå®Œæ•´çš„æ¨¡å‹æ¯”è¼ƒ"""
        
        print("ğŸš€ é–‹å§‹å®Œæ•´çš„PyTorchæ¨¡å‹æ¯”è¼ƒå¯¦é©—...")
        print("=" * 80)
        
        # 1. è¼‰å…¥å’Œæº–å‚™æ•¸æ“š
        if not self.load_data():
            return
        
        self.create_features()
        self.prepare_data()
        
        # 2. è¨“ç·´æ‰€æœ‰å¯ç”¨æ¨¡å‹
        self.train_random_forest()
        
        if TORCH_AVAILABLE:
            self.train_pytorch_lstm()
            self.train_pytorch_transformer()
        
        # 3. æ¯”è¼ƒæ¨¡å‹æ€§èƒ½
        comparison_df = self.compare_models()
        
        # 4. å¯è¦–åŒ–çµæœ
        self.visualize_predictions()
        self.plot_performance_comparison()
        
        # 5. ç”Ÿæˆå ±å‘Š
        self.generate_comparison_report(comparison_df)
        
        print("\nğŸ‰ PyTorchæ¨¡å‹æ¯”è¼ƒå¯¦é©—å®Œæˆï¼")
        return comparison_df
    
    def generate_comparison_report(self, comparison_df):
        """ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒå ±å‘Š"""
        
        report = []
        report.append(f"# ğŸš€ {self.pool_name} æ± å­ç´”PyTorchæ¨¡å‹æ¯”è¼ƒå ±å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ™‚é–“: {pd.Timestamp.now()}")
        report.append(f"æ¡†æ¶: ç´”PyTorchå¯¦ç¾ (Random Forest + PyTorch LSTM + PyTorch Transformer)")
        report.append(f"æ•¸æ“šæœŸé–“: 365å¤©")
        report.append(f"é æ¸¬ç›®æ¨™: æœªä¾†6å°æ™‚Virtual Priceæ”¶ç›Šç‡")
        report.append("")
        
        # æœ€ä½³æ¨¡å‹
        if len(comparison_df) > 0:
            best_model = comparison_df.iloc[0]
            report.append("ğŸ† æœ€ä½³è¡¨ç¾æ¨¡å‹:")
            report.append(f"æ¨¡å‹: {best_model['Model']}")
            report.append(f"æ¸¬è©¦æº–ç¢ºç‡: {best_model['Test Direction Accuracy (%)']:.2f}%")
            report.append(f"æ¸¬è©¦MAE: {best_model['Test MAE']:.4f}%")
            report.append("")
            
            # æ¨¡å‹æ’å
            report.append("ğŸ“Š æ¨¡å‹æ’å (æŒ‰æº–ç¢ºç‡):")
            for idx, row in comparison_df.iterrows():
                rank = idx + 1
                model = row['Model']
                acc = row['Test Direction Accuracy (%)']
                mae = row['Test MAE']
                report.append(f"{rank}. {model}: {acc:.2f}% (MAE: {mae:.4f})")
            
            report.append("")
            report.append("ğŸ’¡ å»ºè­°:")
            
            if best_model['Test Direction Accuracy (%)'] > 70:
                report.append("âœ… æœ€ä½³æ¨¡å‹è¡¨ç¾å„ªç§€ï¼Œå»ºè­°ç”¨æ–¼å¯¦éš›é æ¸¬")
            elif best_model['Test Direction Accuracy (%)'] > 60:
                report.append("âš–ï¸ æœ€ä½³æ¨¡å‹è¡¨ç¾ä¸­ç­‰ï¼Œå»ºè­°è¬¹æ…ä½¿ç”¨")
            else:
                report.append("âš ï¸ æ‰€æœ‰æ¨¡å‹è¡¨ç¾ä¸€èˆ¬ï¼Œå»ºè­°æ”¹é€²ç‰¹å¾µå·¥ç¨‹")
        
        # ä¿å­˜å ±å‘Š
        report_content = "\n".join(report)
        with open(f'{self.pool_name}_pytorch_comparison_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜: {self.pool_name}_pytorch_comparison_report.txt")

def demo_pytorch_model_comparison():
    """æ¼”ç¤ºç´”PyTorchæ¨¡å‹æ¯”è¼ƒ"""
    
    print("ğŸš€ Curve Virtual Priceé æ¸¬ - ç´”PyTorchæ¨¡å‹æ¯”è¼ƒæ¼”ç¤º")
    print("=" * 80)
    
    # å‰µå»ºæ¯”è¼ƒå™¨
    comparator = PyTorchModelComparison(pool_name='3pool', sequence_length=24)
    
    # é‹è¡Œå®Œæ•´æ¯”è¼ƒ
    comparison_df = comparator.run_complete_comparison()
    
    if comparison_df is not None:
        print(f"\nğŸ¯ å¯¦é©—ç¸½çµ:")
        print(f"æœ€ä½³æ¨¡å‹: {comparison_df.iloc[0]['Model']}")
        print(f"æœ€é«˜æº–ç¢ºç‡: {comparison_df.iloc[0]['Test Direction Accuracy (%)']:.2f}%")
        print(f"æ¡†æ¶: ç´”PyTorchå¯¦ç¾ (çµ±ä¸€æ¡†æ¶)")
        
        # ä¿å­˜æ¯”è¼ƒçµæœ
        comparison_df.to_csv(f'3pool_pytorch_comparison_results.csv', index=False)
        print("ğŸ’¾ æ¯”è¼ƒçµæœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶")

if __name__ == "__main__":
    demo_pytorch_model_comparison() 